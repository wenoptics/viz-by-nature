{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n",
      "  1\n",
      "   1\n",
      "  1\n",
      "   1\n",
      "   1\n"
     ]
    }
   ],
   "source": [
    "# Lets make a tree structure out of this\n",
    "class Tree:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def put_left(self, value):\n",
    "        if self.left:\n",
    "            self.left.value = value\n",
    "        else:\n",
    "            self.left = Tree(value)\n",
    "        return self.left\n",
    "            \n",
    "    def put_right(self, value):\n",
    "        if self.right: \n",
    "            self.right.value = value\n",
    "        else: \n",
    "            self.right = Tree(value)\n",
    "        return self.right\n",
    "    \n",
    "    def walk(self, cb_f=None, cb_l=None, lvl=0):\n",
    "        if cb_f: cb_f(self, lvl)\n",
    "        if self.left: self.left.walk(cb_f, cb_l, lvl+1)\n",
    "        if self.right: self.right.walk(cb_f, cb_l, lvl+1)\n",
    "        if cb_l: cb_l(self, lvl)\n",
    "\n",
    "\n",
    "root = Tree(2)\n",
    "\n",
    "for dks in ['DD', 'DK', 'KK']:\n",
    "    node = root\n",
    "    for i in range(len(dks)):\n",
    "        d_or_k = dks[i]\n",
    "        if d_or_k == 'K':\n",
    "            node = node.put_left(1)\n",
    "        elif d_or_k == 'D':\n",
    "            node = node.put_right(1)\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "def print_tree(root):\n",
    "    def _print_l(node, lv): \n",
    "        print(\n",
    "            ''.join([' ']*lv*1),\n",
    "            node.value\n",
    "        )\n",
    "    root.walk(_print_l)\n",
    "    \n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l[+l[+l][-l]][-l[-l]]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tree_to_lstring(node):\n",
    "    if not node:\n",
    "        return\n",
    "    ret = ('l')\n",
    "    if node.right:\n",
    "        ret += ('[+')\n",
    "        ret += tree_to_lstring(node.right)\n",
    "        ret += (']')\n",
    "    if node.left:\n",
    "        ret += ('[-')\n",
    "        ret += tree_to_lstring(node.left)\n",
    "        ret += (']')\n",
    "    return ret\n",
    "\n",
    "tree_to_lstring(root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Statistical Phrase-Based Translation_535',\n",
       "  [[2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Minimum Error Rate Training In Statistical Machine Translation_449',\n",
       "  [[2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['A Maximum-Entropy-Inspired Parser_432',\n",
       "  [[2000, 0],\n",
       "   [2001, 1],\n",
       "   [2002, 0],\n",
       "   [2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['A Hierarchical Phrase-Based Model For Statistical Machine Translation_339',\n",
       "  [[2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms_277',\n",
       "  [[2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking_271',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Thumbs Up? Sentiment Classification Using Machine Learning Techniques_234',\n",
       "  [[2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['CoNLL-X Shared Task On Multilingual Dependency Parsing_198',\n",
       "  [[2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency_183',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " [\"What's In A Translation Rule?_172\",\n",
       "  [[2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Statistical Significance Tests For Machine Translation Evaluation_168',\n",
       "  [[2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Discriminative Training And Maximum Entropy Models For Statistical Machine Translation_165',\n",
       "  [[2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Improving Machine Learning Approaches To Coreference Resolution_162',\n",
       "  [[2002, 0],\n",
       "   [2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Online Large-Margin Training Of Dependency Parsers_155',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 1],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Online Learning Of Approximate Dependency Parsing Algorithms_153',\n",
       "  [[2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 1],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Vector-based Models of Semantic Composition_150',\n",
       "  [[2008, 0], [2009, 1], [2010, 1], [2011, 1], [2013, 0]]],\n",
       " ['Improved Inference for Unlexicalized Parsing_148',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network_148',\n",
       "  [[2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Automatic Labeling Of Semantic Roles_143',\n",
       "  [[2002, 1],\n",
       "   [2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Scalable Inference And Training Of Context-Rich Syntactic Translation Models_138',\n",
       "  [[2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 0]]],\n",
       " ['A Syntax-Based Statistical Translation Model_133',\n",
       "  [[2001, 0],\n",
       "   [2002, 0],\n",
       "   [2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 1]]],\n",
       " ['New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures, And The Voted Perceptron_132',\n",
       "  [[2002, 0],\n",
       "   [2003, 1],\n",
       "   [2004, 1],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling_129',\n",
       "  [[2006, 1],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['ROUGE: A Package For Automatic Evaluation Of Summaries_127',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Simple Semi-supervised Dependency Parsing_118',\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments_113',\n",
       "  [[2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models_112',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Paraphrasing With Bilingual Parallel Corpora_110',\n",
       "  [[2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification_108',\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Forest Reranking: Discriminative Parsing with Non-Local Features_108',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]]],\n",
       " ['Contrastive Estimation: Training Log-Linear Models On Unlabeled Data_107',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 1],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Tree-To-String Alignment Template For Statistical Machine Translation_107',\n",
       "  [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts_106',\n",
       "  [[2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['A fully Bayesian approach to unsupervised part-of-speech tagging_103',\n",
       "  [[2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Distant supervision for relation extraction without labeled data_100',\n",
       "  [[2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['A Phrase-Based, Joint Probability Model For Statistical Machine Translation_99',\n",
       "  [[2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 1],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Learning Surface Text Patterns For A Question Answering System_96',\n",
       "  [[2002, 0],\n",
       "   [2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 1],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 1]]],\n",
       " ['Calibrating Features For Semantic Role Labeling_94',\n",
       "  [[2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['Better K-Best Parsing_92',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['Alignment By Agreement_92',\n",
       "  [[2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Syntax Augmented Machine Translation Via Chart Parsing_91',\n",
       "  [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]]],\n",
       " ['Online Large-Margin Training of Syntactic and Structural Translation Features_89',\n",
       "  [[2009, 0], [2010, 1], [2011, 0], [2013, 0]]],\n",
       " ['A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model_89',\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment_88',\n",
       "  [[2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics_87',\n",
       "  [[2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Shallow Semantic Parsing Using Support Vector Machines_86',\n",
       "  [[2004, 1],\n",
       "   [2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['TextRank: Bringing Order Into Texts_85',\n",
       "  [[2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 1],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Dependency Tree Kernels For Relation Extraction_85',\n",
       "  [[2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['Accurate Unlexicalized Parsing_83',\n",
       "  [[2004, 1],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Efficient Third-Order Dependency Parsers_83',\n",
       "  [[2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Improving Machine Translation Performance By Exploiting Non-Parallel Corpora_83',\n",
       "  [[2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 1]]],\n",
       " ['Dependency Treelet Translation: Syntactically Informed Phrasal SMT_81',\n",
       "  [[2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['Effective Self-Training For Parsing_81',\n",
       "  [[2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Moses: Open Source Toolkit for Statistical Machine Translation_80',\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Forest Rescoring: Faster Decoding with Integrated Language Models_79',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " [\"Why Doesn't EM Find Good HMM POS-Taggers?_79\",\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Extracting Paraphrases From A Parallel Corpus_78',\n",
       "  [[2002, 0],\n",
       "   [2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 1]]],\n",
       " ['Forest-Based Translation_78',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space_78',\n",
       "  [[2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['OntoNotes: The 90% Solution_78',\n",
       "  [[2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['A Statistical Approach To Machine Translation_78',\n",
       "  [[1991, 1],\n",
       "   [1992, 0],\n",
       "   [1993, 0],\n",
       "   [1994, 1],\n",
       "   [1995, 1],\n",
       "   [1996, 0],\n",
       "   [1997, 0],\n",
       "   [1998, 0],\n",
       "   [1999, 0],\n",
       "   [2000, 0],\n",
       "   [2001, 0],\n",
       "   [2002, 1],\n",
       "   [2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Domain Adaptation With Structural Correspondence Learning_77',\n",
       "  [[2007, 1], [2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Integrating Graph-Based and Transition-Based Dependency Parsers_77',\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing_76',\n",
       "  [[2009, 1], [2010, 1], [2011, 0], [2013, 0]]],\n",
       " ['Syntactic Constraints on Paraphrases Extracted from Parallel Corpora_76',\n",
       "  [[2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Kernel Methods For Relation Extraction_76',\n",
       "  [[2004, 1],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Large Language Models in Machine Translation_74',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Pseudo-Projective Dependency Parsing_74',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['The Penn Discourse TreeBank 2.0._73',\n",
       "  [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Chunking With Support Vector Machines_73',\n",
       "  [[2002, 1],\n",
       "   [2003, 0],\n",
       "   [2004, 1],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 1]]],\n",
       " ['Prototype-Driven Learning For Sequence Models_73',\n",
       "  [[2007, 1], [2008, 0], [2009, 1], [2010, 0], [2011, 1], [2013, 0]]],\n",
       " ['11,001 New Features for Statistical Machine Translation_72',\n",
       "  [[2009, 0], [2010, 0], [2011, 1], [2013, 0]]],\n",
       " ['Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations_72',\n",
       "  [[2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 1]]],\n",
       " ['Phrasal Cohesion And Statistical Machine Translation_70',\n",
       "  [[2003, 1],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 1]]],\n",
       " ['Experiments with a Higher-Order Projective Dependency Parser_70',\n",
       "  [[2008, 0], [2009, 0], [2010, 1], [2011, 0], [2013, 0]]],\n",
       " ['Dynamic Programming for Linear-Time Incremental Parsing_69',\n",
       "  [[2010, 0], [2011, 1], [2013, 1]]],\n",
       " ['SPMT: Statistical Machine Translation With Syntactified Target Language Phrases_69',\n",
       "  [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation_68',\n",
       "  [[2007, 0], [2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 1]]],\n",
       " ['Identifying Anaphoric And Non-Anaphoric Noun Phrases To Improve Coreference Resolution_68',\n",
       "  [[2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0]]],\n",
       " ['A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents_68',\n",
       "  [[2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 1],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 1]]],\n",
       " ['Minimum Bayes-Risk Decoding For Statistical Machine Translation_68',\n",
       "  [[2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['A Shortest Path Dependency Kernel For Relation Extraction_67',\n",
       "  [[2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Factored Translation Models_67',\n",
       "  [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Learning Non-Isomorphic Tree Mappings For Machine Translation_67',\n",
       "  [[2004, 1],\n",
       "   [2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 1],\n",
       "   [2013, 0]]],\n",
       " ['Learning Extraction Patterns For Subjective Expressions_67',\n",
       "  [[2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Discriminative Reranking For Natural Language Parsing_67',\n",
       "  [[2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Arabic Tokenization, Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop_67',\n",
       "  [[2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources_66',\n",
       "  [[2005, 0],\n",
       "   [2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['A Simple, Similarity-based Model for Selectional Preferences_66',\n",
       "  [[2008, 0], [2009, 0], [2010, 1], [2011, 1], [2013, 1]]],\n",
       " ['Incremental Parsing With The Perceptron Algorithm_66',\n",
       "  [[2005, 1],\n",
       "   [2006, 1],\n",
       "   [2007, 1],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Parsing The WSJ Using CCG And Log-Linear Models_66',\n",
       "  [[2004, 0],\n",
       "   [2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 1],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Immediate-Head Parsing For Language Models_65',\n",
       "  [[2002, 1],\n",
       "   [2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 1],\n",
       "   [2011, 1],\n",
       "   [2013, 1]]],\n",
       " ['Semantic Taxonomy Induction From Heterogenous Evidence_65',\n",
       "  [[2007, 0], [2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Weakly Supervised Learning for Hedge Classification in Scientific Literature_65',\n",
       "  [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Probabilistic CFG With Latent Annotations_63',\n",
       "  [[2005, 1],\n",
       "   [2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Introduction To The CoNLL-2000 Shared Task: Chunking_62',\n",
       "  [[2000, 0],\n",
       "   [2001, 0],\n",
       "   [2002, 0],\n",
       "   [2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Improving A Statistical MT System With Automatically Learned Rewrite Patterns_62',\n",
       "  [[2005, 1],\n",
       "   [2006, 0],\n",
       "   [2007, 1],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 1],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['The Second Release Of The RASP System_61',\n",
       "  [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Contextual Dependencies In Unsupervised Word Segmentation_60',\n",
       "  [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]],\n",
       " ['Learning A Translation Lexicon From Monolingual Corpora_60',\n",
       "  [[2003, 0],\n",
       "   [2004, 0],\n",
       "   [2005, 0],\n",
       "   [2006, 0],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 1],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]],\n",
       " ['Determining The Sentiment Of Opinions_60',\n",
       "  [[2006, 1],\n",
       "   [2007, 0],\n",
       "   [2008, 0],\n",
       "   [2009, 0],\n",
       "   [2010, 0],\n",
       "   [2011, 0],\n",
       "   [2013, 0]]]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "citation_data = json.load(open('tree_final.json'))\n",
    "citation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 101\n"
     ]
    }
   ],
   "source": [
    "citation_data_dict = {}\n",
    "\n",
    "for arti in citation_data:\n",
    "    title, data = arti\n",
    "    if title in citation_data_dict:\n",
    "        print('[W] Duplicated:', title, data)\n",
    "        print('    With      :', title, citation_data_dict[data], end='\\n\\n')\n",
    "    citation_data_dict[title] = data\n",
    "    \n",
    "print(len(citation_data_dict), len(citation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Statistical Phrase-Based Translation_535': [[2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Minimum Error Rate Training In Statistical Machine Translation_449': [[2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Maximum-Entropy-Inspired Parser_432': [[2000, 0], [2001, 1], [2002, 0], [2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Hierarchical Phrase-Based Model For Statistical Machine Translation_339': [[2006, 0], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms_277': [[2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking_271': [[2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Thumbs Up? Sentiment Classification Using Machine Learning Techniques_234': [[2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'CoNLL-X Shared Task On Multilingual Dependency Parsing_198': [[2006, 0], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency_183': [[2005, 1], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], \"What's In A Translation Rule?_172\": [[2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Statistical Significance Tests For Machine Translation Evaluation_168': [[2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Discriminative Training And Maximum Entropy Models For Statistical Machine Translation_165': [[2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Improving Machine Learning Approaches To Coreference Resolution_162': [[2002, 0], [2003, 0], [2004, 0], [2005, 1], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Online Large-Margin Training Of Dependency Parsers_155': [[2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 1], [2010, 0], [2011, 0], [2013, 0]], 'Online Learning Of Approximate Dependency Parsing Algorithms_153': [[2006, 0], [2007, 1], [2008, 1], [2009, 0], [2010, 1], [2011, 0], [2013, 0]], 'Vector-based Models of Semantic Composition_150': [[2008, 0], [2009, 1], [2010, 1], [2011, 1], [2013, 0]], 'Improved Inference for Unlexicalized Parsing_148': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network_148': [[2004, 0], [2005, 0], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Automatic Labeling Of Semantic Roles_143': [[2002, 1], [2003, 1], [2004, 0], [2005, 1], [2006, 0], [2007, 1], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Scalable Inference And Training Of Context-Rich Syntactic Translation Models_138': [[2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'A Syntax-Based Statistical Translation Model_133': [[2001, 0], [2002, 0], [2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures, And The Voted Perceptron_132': [[2002, 0], [2003, 1], [2004, 1], [2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling_129': [[2006, 1], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'ROUGE: A Package For Automatic Evaluation Of Summaries_127': [[2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Simple Semi-supervised Dependency Parsing_118': [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments_113': [[2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models_112': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Paraphrasing With Bilingual Parallel Corpora_110': [[2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification_108': [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Forest Reranking: Discriminative Parsing with Non-Local Features_108': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Contrastive Estimation: Training Log-Linear Models On Unlabeled Data_107': [[2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 1], [2010, 0], [2011, 0], [2013, 0]], 'Tree-To-String Alignment Template For Statistical Machine Translation_107': [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts_106': [[2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A fully Bayesian approach to unsupervised part-of-speech tagging_103': [[2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 0]], 'Distant supervision for relation extraction without labeled data_100': [[2010, 0], [2011, 0], [2013, 0]], 'A Phrase-Based, Joint Probability Model For Statistical Machine Translation_99': [[2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 1], [2010, 0], [2011, 0], [2013, 0]], 'Learning Surface Text Patterns For A Question Answering System_96': [[2002, 0], [2003, 0], [2004, 0], [2005, 0], [2006, 1], [2007, 1], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Calibrating Features For Semantic Role Labeling_94': [[2005, 0], [2006, 0], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Better K-Best Parsing_92': [[2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Alignment By Agreement_92': [[2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Syntax Augmented Machine Translation Via Chart Parsing_91': [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Online Large-Margin Training of Syntactic and Structural Translation Features_89': [[2009, 0], [2010, 1], [2011, 0], [2013, 0]], 'A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model_89': [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment_88': [[2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 1], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics_87': [[2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Shallow Semantic Parsing Using Support Vector Machines_86': [[2004, 1], [2005, 0], [2006, 1], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'TextRank: Bringing Order Into Texts_85': [[2005, 0], [2006, 1], [2007, 1], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Dependency Tree Kernels For Relation Extraction_85': [[2005, 0], [2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Accurate Unlexicalized Parsing_83': [[2004, 1], [2005, 0], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Efficient Third-Order Dependency Parsers_83': [[2010, 0], [2011, 0], [2013, 0]], 'Improving Machine Translation Performance By Exploiting Non-Parallel Corpora_83': [[2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Dependency Treelet Translation: Syntactically Informed Phrasal SMT_81': [[2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Effective Self-Training For Parsing_81': [[2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Moses: Open Source Toolkit for Statistical Machine Translation_80': [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Forest Rescoring: Faster Decoding with Integrated Language Models_79': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], \"Why Doesn't EM Find Good HMM POS-Taggers?_79\": [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Extracting Paraphrases From A Parallel Corpus_78': [[2002, 0], [2003, 0], [2004, 0], [2005, 1], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Forest-Based Translation_78': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space_78': [[2010, 0], [2011, 0], [2013, 0]], 'OntoNotes: The 90% Solution_78': [[2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Statistical Approach To Machine Translation_78': [[1991, 1], [1992, 0], [1993, 0], [1994, 1], [1995, 1], [1996, 0], [1997, 0], [1998, 0], [1999, 0], [2000, 0], [2001, 0], [2002, 1], [2003, 0], [2004, 0], [2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Domain Adaptation With Structural Correspondence Learning_77': [[2007, 1], [2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 0]], 'Integrating Graph-Based and Transition-Based Dependency Parsers_77': [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing_76': [[2009, 1], [2010, 1], [2011, 0], [2013, 0]], 'Syntactic Constraints on Paraphrases Extracted from Parallel Corpora_76': [[2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Kernel Methods For Relation Extraction_76': [[2004, 1], [2005, 1], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Large Language Models in Machine Translation_74': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Pseudo-Projective Dependency Parsing_74': [[2005, 1], [2006, 0], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'The Penn Discourse TreeBank 2.0._73': [[2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Chunking With Support Vector Machines_73': [[2002, 1], [2003, 0], [2004, 1], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 1]], 'Prototype-Driven Learning For Sequence Models_73': [[2007, 1], [2008, 0], [2009, 1], [2010, 0], [2011, 1], [2013, 0]], '11,001 New Features for Statistical Machine Translation_72': [[2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations_72': [[2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Phrasal Cohesion And Statistical Machine Translation_70': [[2003, 1], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 1]], 'Experiments with a Higher-Order Projective Dependency Parser_70': [[2008, 0], [2009, 0], [2010, 1], [2011, 0], [2013, 0]], 'Dynamic Programming for Linear-Time Incremental Parsing_69': [[2010, 0], [2011, 1], [2013, 1]], 'SPMT: Statistical Machine Translation With Syntactified Target Language Phrases_69': [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation_68': [[2007, 0], [2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 1]], 'Identifying Anaphoric And Non-Anaphoric Noun Phrases To Improve Coreference Resolution_68': [[2003, 0], [2004, 0], [2005, 1], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0]], 'A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents_68': [[2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 1], [2008, 0], [2009, 1], [2010, 0], [2011, 0], [2013, 1]], 'Minimum Bayes-Risk Decoding For Statistical Machine Translation_68': [[2004, 0], [2005, 0], [2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Shortest Path Dependency Kernel For Relation Extraction_67': [[2006, 1], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Factored Translation Models_67': [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Learning Non-Isomorphic Tree Mappings For Machine Translation_67': [[2004, 1], [2005, 0], [2006, 1], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 1], [2013, 0]], 'Learning Extraction Patterns For Subjective Expressions_67': [[2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Discriminative Reranking For Natural Language Parsing_67': [[2005, 0], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Arabic Tokenization, Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop_67': [[2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources_66': [[2005, 0], [2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'A Simple, Similarity-based Model for Selectional Preferences_66': [[2008, 0], [2009, 0], [2010, 1], [2011, 1], [2013, 1]], 'Incremental Parsing With The Perceptron Algorithm_66': [[2005, 1], [2006, 1], [2007, 1], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Parsing The WSJ Using CCG And Log-Linear Models_66': [[2004, 0], [2005, 1], [2006, 0], [2007, 0], [2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Immediate-Head Parsing For Language Models_65': [[2002, 1], [2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 1], [2011, 1], [2013, 1]], 'Semantic Taxonomy Induction From Heterogenous Evidence_65': [[2007, 0], [2008, 1], [2009, 1], [2010, 0], [2011, 0], [2013, 0]], 'Weakly Supervised Learning for Hedge Classification in Scientific Literature_65': [[2008, 1], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Probabilistic CFG With Latent Annotations_63': [[2005, 1], [2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Introduction To The CoNLL-2000 Shared Task: Chunking_62': [[2000, 0], [2001, 0], [2002, 0], [2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Improving A Statistical MT System With Automatically Learned Rewrite Patterns_62': [[2005, 1], [2006, 0], [2007, 1], [2008, 0], [2009, 0], [2010, 1], [2011, 0], [2013, 0]], 'The Second Release Of The RASP System_61': [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Contextual Dependencies In Unsupervised Word Segmentation_60': [[2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]], 'Learning A Translation Lexicon From Monolingual Corpora_60': [[2003, 0], [2004, 0], [2005, 0], [2006, 0], [2007, 0], [2008, 0], [2009, 0], [2010, 1], [2011, 0], [2013, 0]], 'Determining The Sentiment Of Opinions_60': [[2006, 1], [2007, 0], [2008, 0], [2009, 0], [2010, 0], [2011, 0], [2013, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(citation_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  0.,  4.,  0.,  0., 18.,  0., 13.,  0.,  0., 13.,  0.,  0.,\n",
       "        17.,  0., 11.,  0.,  0., 10.,  0.,  0.,  7.,  0.,  1.,  0.,  0.,\n",
       "         2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
       " array([ 3.  ,  3.38,  3.76,  4.14,  4.52,  4.9 ,  5.28,  5.66,  6.04,\n",
       "         6.42,  6.8 ,  7.18,  7.56,  7.94,  8.32,  8.7 ,  9.08,  9.46,\n",
       "         9.84, 10.22, 10.6 , 10.98, 11.36, 11.74, 12.12, 12.5 , 12.88,\n",
       "        13.26, 13.64, 14.02, 14.4 , 14.78, 15.16, 15.54, 15.92, 16.3 ,\n",
       "        16.68, 17.06, 17.44, 17.82, 18.2 , 18.58, 18.96, 19.34, 19.72,\n",
       "        20.1 , 20.48, 20.86, 21.24, 21.62, 22.  ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([(len(citation_data_dict[d])) for d in citation_data_dict], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l[+l[+l[+l[+l[-l[-l[-l[-l]]]]][-l[-l[-l]]]][-l[+l[-l[+l[+l[-l[-l[-l[-l]]]]]]]][-l[-l[-l[-l[-l[-l]]]]]]]][-l[+l[+l[-l[-l[-l[+l[-l]]]]]][-l[+l[-l[-l[-l[-l]]]]][-l[+l[-l[-l]]][-l[-l[-l[-l[+l[+l]]]]]]]]][-l[+l[+l[-l[-l[-l[-l[-l[-l[+l[-l[-l[+l[-l[-l[-l[-l[-l[-l[-l]]]]]]]]]]]]]]]]]][-l[-l[+l[-l]][-l[-l[-l]]]]]][-l[+l[-l[-l[-l]]]][-l[-l[+l[-l[-l[-l]]]][-l[-l[+l[+l[+l]]][-l[+l][-l]]]]]]]]]][-l[+l[+l[+l[-l[-l[-l[-l[-l[-l[-l]]]]]]]][-l[+l[-l[-l]]][-l[+l][-l]]]][-l[+l[-l[-l[-l[-l[-l[-l[-l[-l[-l]]]]]]]]]][-l[+l[-l[-l[-l[-l]]]]][-l[+l][-l[+l[-l]][-l[-l]]]]]]][-l[+l[+l[+l]][-l[+l[-l[-l[-l[-l]]]]][-l[+l[-l]][-l[-l[-l[-l[-l[-l[+l]]]]]]]]]][-l[+l[-l[+l[-l[-l[-l[-l[+l][-l]]]]]][-l[+l[-l]][-l[-l[-l]]]]]][-l[+l[+l[+l[-l[-l[-l[+l]]]]][-l[-l[-l[-l]]]]][-l[+l[-l[-l[+l]]]]]][-l[+l[-l]][-l[-l[+l[-l[-l]]][-l[-l[-l[-l[-l[-l]]]]]]]]]]]]]\n"
     ]
    }
   ],
   "source": [
    "def generate_tree():\n",
    "    root = Tree(None)\n",
    "    for k in citation_data_dict:\n",
    "        node = root\n",
    "        for _, bi in citation_data_dict[k]:\n",
    "            if bi == 0:\n",
    "                node = node.put_left(1)\n",
    "            elif bi == 1:\n",
    "                node = node.put_right(1)\n",
    "            else:\n",
    "                assert False\n",
    "    return root\n",
    "\n",
    "root = generate_tree()\n",
    "print(tree_to_lstring(root))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "function LogXY(x, y) {\n",
       "  return Math.log(y) / Math.log(x);\n",
       "}\n",
       "\n",
       "// ----------------------------------------------------\n",
       "// Treer2d by Arnaud Couturier, improve by Grayson Wen\n",
       "var trees2d = {};\n",
       "trees2d.tree = function (canvas2d) {\n",
       "    this.canvas2d = canvas2d;\n",
       "    this.ctx2d = canvas2d.getContext(\"2d\");\n",
       "    this.string = \"lL[+L][-L]\";\n",
       "    this.branchTexture;\n",
       "    this.treePosY = 50;\n",
       "    this.iterations = 10;\n",
       "    this.angleMean = 0.34;\n",
       "    this.length = 29;\n",
       "    this.lengthReduction = 0.28;\n",
       "    this.thickness = 1;\n",
       "    this.thicknessReduction = 1;\n",
       "    this.rules = {\n",
       "        L: {developsInto: [\"l\", \"+lL\", \"-lL\", \"L[+LL][-LL]l\", \"[+L][-L]\", \"L[+lLL]\", \"L[-lLL]\"]},\n",
       "        l: {developsInto: [\"l\"]},\n",
       "        \"[\": {developsInto: [\"[\"]},\n",
       "        \"]\": {developsInto: [\"]\"]},\n",
       "        \"+\": {developsInto: [\"+\"]},\n",
       "        \"-\": {developsInto: [\"-\"]}\n",
       "    };\n",
       "    this.leafTextures = [];\n",
       "    this.leafScaleVariation = 0.5;\n",
       "    this.leafMinDepth = 4;\n",
       "    this.leafProba = 0.5;\n",
       "    this.leafScale = 1;\n",
       "    this.leafTotalPerBranch = 1;\n",
       "    this.leafProbaLighterMult = 0.5\n",
       "    this.shadowProba = 0;\n",
       "    this.shadowAlpha = 0.025;\n",
       "    this.shadowRadius = 0;\n",
       "    this.leftColor = '#229b71';\n",
       "    this.rightColor = '#982b6f';\n",
       "};\n",
       "trees2d.tree.prototype.addLeafTexture = function (a, b, c) {\n",
       "    this.leafTextures.push({img: a, targetWidthInPixel: b, relativeProba: c})\n",
       "};\n",
       "trees2d.tree.prototype.generateLString = function () {\n",
       "    // L-string expansion\n",
       "    var _str = this.string;\n",
       "    var lstring;\n",
       "\n",
       "    for (var m = 0; m < this.iterations; m++) {\n",
       "        lstring = \"\";\n",
       "        for (var s = 0; s < _str.length; s++) {\n",
       "            var _ch = _str[s];\n",
       "            var _developsInto = this.rules[_ch].developsInto;\n",
       "            if (_developsInto.length <= 0) {\n",
       "                continue\n",
       "            }\n",
       "            // Randomly pick one of the devInto\n",
       "            var a = _developsInto[parseInt(Math.random() * _developsInto.length)];\n",
       "\n",
       "            lstring += a\n",
       "        }\n",
       "        _str = lstring\n",
       "    }\n",
       "    return lstring\n",
       "};\n",
       "trees2d.tree.prototype.removeLeafTexture = function (a) {\n",
       "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
       "        var c = this.leafTextures[b];\n",
       "        if (c.img == a) {\n",
       "            this.leafTextures.splice(b, 1);\n",
       "            break\n",
       "        }\n",
       "    }\n",
       "};\n",
       "trees2d.tree.prototype.hasLeafTexture = function (a) {\n",
       "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
       "        var c = this.leafTextures[b];\n",
       "        if (c.img == a) {\n",
       "            return true\n",
       "        }\n",
       "    }\n",
       "    return false\n",
       "};\n",
       "trees2d.tree.prototype.totalLeafTextures = function () {\n",
       "    return this.leafTextures.length\n",
       "};\n",
       "trees2d.tree.prototype.draw = function (lstring) {\n",
       "    for (var s = 0; s < this.leafTextures.length; s++) {\n",
       "        var l = this.leafTextures[s];\n",
       "        if (!l.img.complete) {\n",
       "            return l.img.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
       "        }\n",
       "        if (l.img.naturalWidth === 0 || l.img.naturalHeight === 0) {\n",
       "            return l.img.src.substring(0, 100) + \" is not a valid image\"\n",
       "        }\n",
       "    }\n",
       "    if (this.branchTexture) {\n",
       "        if (!this.branchTexture.complete) {\n",
       "            return this.branchTexture.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
       "        }\n",
       "        if (this.branchTexture.naturalWidth === 0 || this.branchTexture.naturalHeight === 0) {\n",
       "            return this.branchTexture.src.substring(0, 100) + \" is not a valid image\"\n",
       "        }\n",
       "    }\n",
       "    var ctx = this.ctx2d;\n",
       "    var err;\n",
       "    ctx.setTransform(1, 0, 0, 1, 0, 0);\n",
       "    ctx.save();\n",
       "    try {\n",
       "\n",
       "        var w = 0.1;\n",
       "        var f = 1 / this.iterations;\n",
       "        var strokeStyle = this.branchTexture ? this.ctx2d.createPattern(this.branchTexture, \"repeat\") : 'black';\n",
       "        ctx.strokeStyle = strokeStyle;\n",
       "        ctx.lineWidth = this.thickness;\n",
       "        ctx.lineCap = \"round\";\n",
       "        var p = 1;\n",
       "        var h = 0;\n",
       "        this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p))\n",
       "        ctx.clearRect(0, 0, this.canvas2d.width, this.canvas2d.height);\n",
       "        ctx.translate(this.canvas2d.width / 2, this.canvas2d.height - this.treePosY);\n",
       "        for (var s = 0; s < lstring.length; s++) {\n",
       "            switch (lstring[s]) {\n",
       "                case \"l\":\n",
       "                    ctx.beginPath();\n",
       "                    ctx.moveTo(0, 0);\n",
       "                    ctx.quadraticCurveTo(0, - this.length / 2, 0, -this.length);\n",
       "                    ctx.stroke();\n",
       "\n",
       "                    if (Math.random() <= this.shadowProba) {\n",
       "                        // Draw dark shadow\n",
       "                        ctx.save();\n",
       "                        ctx.globalCompositeOperation = \"source-atop\";\n",
       "                        ctx.globalAlpha = this.shadowAlpha;\n",
       "                        ctx.beginPath();\n",
       "                        ctx.arc(0, 0, this.shadowRadius, 0, Math.PI * 2, false);\n",
       "                        ctx.fill();\n",
       "                        ctx.restore()\n",
       "                    }\n",
       "                    h++;\n",
       "                    ctx.translate(0, -this.length);\n",
       "\n",
       "                    // Draw leaf here\n",
       "                    if (this.leafTextures.length > 0 && p >= this.leafMinDepth && Math.random() <= this.leafProba) {\n",
       "                        for (var q = 0; q < this.leafTotalPerBranch; q++) {\n",
       "                            ctx.save();\n",
       "                            if (Math.random() <= (p / this.iterations * this.leafProbaLighterMult)) {\n",
       "                                ctx.globalCompositeOperation = \"lighter\"\n",
       "                            }\n",
       "                            // Randomly pick a load leaf texture\n",
       "                            var _leafTexture = this.leafTextures[parseInt(Math.random() * this.leafTextures.length)];\n",
       "                            var u = _leafTexture.targetWidthInPixel / _leafTexture.img.naturalWidth * this.leafScale;\n",
       "\n",
       "                            ctx.scale(\n",
       "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation),\n",
       "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation)\n",
       "                            );\n",
       "                            ctx.rotate(Math.random() * Math.PI * 2);\n",
       "                            ctx.drawImage(_leafTexture.img, 0, 0);\n",
       "                            ctx.restore()\n",
       "                        }\n",
       "                    }\n",
       "                    break;\n",
       "                case \"+\":\n",
       "                    // Rotate left with variation\n",
       "                    ctx.strokeStyle = this.leftColor;\n",
       "                    ctx.rotate(this.angleMean);\n",
       "                    break;\n",
       "                case \"-\":\n",
       "                    // Rotate right with variation\n",
       "                    ctx.strokeStyle = this.rightColor;\n",
       "                    ctx.rotate(-this.angleMean);\n",
       "                    break;\n",
       "                case \"[\":\n",
       "                    ctx.save();\n",
       "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p+1));\n",
       "                    p++;\n",
       "                    w += f;\n",
       "                    ctx.lineWidth *= this.thicknessReduction;\n",
       "                    break;\n",
       "                case \"]\":\n",
       "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p-1));\n",
       "                    p--;\n",
       "                    w -= f;\n",
       "                    ctx.restore();\n",
       "                    break;\n",
       "            }\n",
       "        }\n",
       "    } catch (e) {\n",
       "        err = e\n",
       "    }\n",
       "    ctx.restore();\n",
       "    return err\n",
       "};\n",
       "\n",
       "// ----------------------------------------------------\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        'dat.gui': 'https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.6/dat.gui.min'\n",
       "    }\n",
       "});\n",
       "\n",
       "(function(element) {\n",
       "    var $canvas = $('<canvas width=\"800\" height=\"500\"></canvas>');\n",
       "    var $controlContainer = $('<div style=\"position:absolute; right:0; top: 0\"></div>');\n",
       "    var $container = $('<div></div>');\n",
       "    $container\n",
       "        .append($controlContainer)\n",
       "        .append($canvas);\n",
       "    element.append($container);\n",
       "    \n",
       "    var treeGenParamValues = {\n",
       "        tp_iterations: {min: 1, max: 16, step: 1}\n",
       "    };\n",
       "    var treeParamValues = {\n",
       "        tp_angleMean: {min: 0, max: Math.PI / 4, step: Math.PI / 4 / 180},\n",
       "        tp_thickness: {min: 1, max: 150, step: .1},\n",
       "        tp_lengthReduction: {min: 0.001, max: 1},\n",
       "        tp_thicknessReduction: {min: .5, max: 1, step: .01}\n",
       "    };\n",
       "    \n",
       "    var tree = new trees2d.tree($canvas[0]);\n",
       "    var data = {\n",
       "        error: ''\n",
       "    }\n",
       "    var methods = {\n",
       "        generate: () => { data.error = tree.draw(methods.lstring) },\n",
       "        lstring: 'l[+l[+l[+l[+l[-l[-l[-l[-l]]]]][-l[-l[-l]]]][-l[+l[-l[+l[+l[-l[-l[-l[-l]]]]]]]][-l[-l[-l[-l[-l[-l]]]]]]]][-l[+l[+l[-l[-l[-l[+l[-l]]]]]][-l[+l[-l[-l[-l[-l]]]]][-l[+l[-l[-l]]][-l[-l[-l[-l[+l[+l]]]]]]]]][-l[+l[+l[-l[-l[-l[-l[-l[-l[+l[-l[-l[+l[-l[-l[-l[-l[-l[-l[-l]]]]]]]]]]]]]]]]]][-l[-l[+l[-l]][-l[-l[-l]]]]]][-l[+l[-l[-l[-l]]]][-l[-l[+l[-l[-l[-l]]]][-l[-l[+l[+l[+l]]][-l[+l][-l]]]]]]]]]][-l[+l[+l[+l[-l[-l[-l[-l[-l[-l[-l]]]]]]]][-l[+l[-l[-l]]][-l[+l][-l]]]][-l[+l[-l[-l[-l[-l[-l[-l[-l[-l[-l]]]]]]]]]][-l[+l[-l[-l[-l[-l]]]]][-l[+l][-l[+l[-l]][-l[-l]]]]]]][-l[+l[+l[+l]][-l[+l[-l[-l[-l[-l]]]]][-l[+l[-l]][-l[-l[-l[-l[-l[-l[+l]]]]]]]]]][-l[+l[-l[+l[-l[-l[-l[-l[+l][-l]]]]]][-l[+l[-l]][-l[-l[-l]]]]]][-l[+l[+l[+l[-l[-l[-l[+l]]]]][-l[-l[-l[-l]]]]][-l[+l[-l[-l[+l]]]]]][-l[+l[-l]][-l[-l[+l[-l[-l]]][-l[-l[-l[-l[-l[-l]]]]]]]]]]]]]'\n",
       "    }\n",
       "    data.error = tree.draw(methods.lstring);\n",
       "    \n",
       "    require(['dat.gui'], (dat) => {   \n",
       "        \n",
       "        var gui = new dat.GUI({autoPlace: false});\n",
       "        $controlContainer.empty()\n",
       "        $controlContainer.append(gui.domElement);\n",
       "        \n",
       "        gui.add(methods, 'lstring').onChange(() => methods.generate());\n",
       "        gui.add(methods, 'generate');\n",
       "        \n",
       "        for (var _i in treeParamValues) {\n",
       "            var p = _i.substring(3, _i.length);\n",
       "            //console.log(tree, _i, p, treeParamValues);\n",
       "            gui.add(tree, p, treeParamValues[_i].min, treeParamValues[_i].max)\n",
       "                .onChange(() => data.error = tree.draw(methods.lstring));\n",
       "        }\n",
       "        \n",
       "        gui.addColor(tree, 'leftColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
       "        gui.addColor(tree, 'rightColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
       "        \n",
       "    })\n",
       "})(element);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "function LogXY(x, y) {\n",
    "  return Math.log(y) / Math.log(x);\n",
    "}\n",
    "\n",
    "// ----------------------------------------------------\n",
    "// Treer2d by Arnaud Couturier, improve by Grayson Wen\n",
    "var trees2d = {};\n",
    "trees2d.tree = function (canvas2d) {\n",
    "    this.canvas2d = canvas2d;\n",
    "    this.ctx2d = canvas2d.getContext(\"2d\");\n",
    "    this.string = \"lL[+L][-L]\";\n",
    "    this.branchTexture;\n",
    "    this.treePosY = 50;\n",
    "    this.iterations = 10;\n",
    "    this.angleMean = 0.34;\n",
    "    this.length = 29;\n",
    "    this.lengthReduction = 0.28;\n",
    "    this.thickness = 1;\n",
    "    this.thicknessReduction = 1;\n",
    "    this.rules = {\n",
    "        L: {developsInto: [\"l\", \"+lL\", \"-lL\", \"L[+LL][-LL]l\", \"[+L][-L]\", \"L[+lLL]\", \"L[-lLL]\"]},\n",
    "        l: {developsInto: [\"l\"]},\n",
    "        \"[\": {developsInto: [\"[\"]},\n",
    "        \"]\": {developsInto: [\"]\"]},\n",
    "        \"+\": {developsInto: [\"+\"]},\n",
    "        \"-\": {developsInto: [\"-\"]}\n",
    "    };\n",
    "    this.leafTextures = [];\n",
    "    this.leafScaleVariation = 0.5;\n",
    "    this.leafMinDepth = 4;\n",
    "    this.leafProba = 0.5;\n",
    "    this.leafScale = 1;\n",
    "    this.leafTotalPerBranch = 1;\n",
    "    this.leafProbaLighterMult = 0.5\n",
    "    this.shadowProba = 0;\n",
    "    this.shadowAlpha = 0.025;\n",
    "    this.shadowRadius = 0;\n",
    "    this.leftColor = '#229b71';\n",
    "    this.rightColor = '#982b6f';\n",
    "};\n",
    "trees2d.tree.prototype.addLeafTexture = function (a, b, c) {\n",
    "    this.leafTextures.push({img: a, targetWidthInPixel: b, relativeProba: c})\n",
    "};\n",
    "trees2d.tree.prototype.generateLString = function () {\n",
    "    // L-string expansion\n",
    "    var _str = this.string;\n",
    "    var lstring;\n",
    "\n",
    "    for (var m = 0; m < this.iterations; m++) {\n",
    "        lstring = \"\";\n",
    "        for (var s = 0; s < _str.length; s++) {\n",
    "            var _ch = _str[s];\n",
    "            var _developsInto = this.rules[_ch].developsInto;\n",
    "            if (_developsInto.length <= 0) {\n",
    "                continue\n",
    "            }\n",
    "            // Randomly pick one of the devInto\n",
    "            var a = _developsInto[parseInt(Math.random() * _developsInto.length)];\n",
    "\n",
    "            lstring += a\n",
    "        }\n",
    "        _str = lstring\n",
    "    }\n",
    "    return lstring\n",
    "};\n",
    "trees2d.tree.prototype.removeLeafTexture = function (a) {\n",
    "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
    "        var c = this.leafTextures[b];\n",
    "        if (c.img == a) {\n",
    "            this.leafTextures.splice(b, 1);\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "};\n",
    "trees2d.tree.prototype.hasLeafTexture = function (a) {\n",
    "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
    "        var c = this.leafTextures[b];\n",
    "        if (c.img == a) {\n",
    "            return true\n",
    "        }\n",
    "    }\n",
    "    return false\n",
    "};\n",
    "trees2d.tree.prototype.totalLeafTextures = function () {\n",
    "    return this.leafTextures.length\n",
    "};\n",
    "trees2d.tree.prototype.draw = function (lstring) {\n",
    "    for (var s = 0; s < this.leafTextures.length; s++) {\n",
    "        var l = this.leafTextures[s];\n",
    "        if (!l.img.complete) {\n",
    "            return l.img.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
    "        }\n",
    "        if (l.img.naturalWidth === 0 || l.img.naturalHeight === 0) {\n",
    "            return l.img.src.substring(0, 100) + \" is not a valid image\"\n",
    "        }\n",
    "    }\n",
    "    if (this.branchTexture) {\n",
    "        if (!this.branchTexture.complete) {\n",
    "            return this.branchTexture.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
    "        }\n",
    "        if (this.branchTexture.naturalWidth === 0 || this.branchTexture.naturalHeight === 0) {\n",
    "            return this.branchTexture.src.substring(0, 100) + \" is not a valid image\"\n",
    "        }\n",
    "    }\n",
    "    var ctx = this.ctx2d;\n",
    "    var err;\n",
    "    ctx.setTransform(1, 0, 0, 1, 0, 0);\n",
    "    ctx.save();\n",
    "    try {\n",
    "\n",
    "        var w = 0.1;\n",
    "        var f = 1 / this.iterations;\n",
    "        var strokeStyle = this.branchTexture ? this.ctx2d.createPattern(this.branchTexture, \"repeat\") : 'black';\n",
    "        ctx.strokeStyle = strokeStyle;\n",
    "        ctx.lineWidth = this.thickness;\n",
    "        ctx.lineCap = \"round\";\n",
    "        var p = 1;\n",
    "        var h = 0;\n",
    "        this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p))\n",
    "        ctx.clearRect(0, 0, this.canvas2d.width, this.canvas2d.height);\n",
    "        ctx.translate(this.canvas2d.width / 2, this.canvas2d.height - this.treePosY);\n",
    "        for (var s = 0; s < lstring.length; s++) {\n",
    "            switch (lstring[s]) {\n",
    "                case \"l\":\n",
    "                    ctx.beginPath();\n",
    "                    ctx.moveTo(0, 0);\n",
    "                    ctx.quadraticCurveTo(0, - this.length / 2, 0, -this.length);\n",
    "                    ctx.stroke();\n",
    "\n",
    "                    if (Math.random() <= this.shadowProba) {\n",
    "                        // Draw dark shadow\n",
    "                        ctx.save();\n",
    "                        ctx.globalCompositeOperation = \"source-atop\";\n",
    "                        ctx.globalAlpha = this.shadowAlpha;\n",
    "                        ctx.beginPath();\n",
    "                        ctx.arc(0, 0, this.shadowRadius, 0, Math.PI * 2, false);\n",
    "                        ctx.fill();\n",
    "                        ctx.restore()\n",
    "                    }\n",
    "                    h++;\n",
    "                    ctx.translate(0, -this.length);\n",
    "\n",
    "                    // Draw leaf here\n",
    "                    if (this.leafTextures.length > 0 && p >= this.leafMinDepth && Math.random() <= this.leafProba) {\n",
    "                        for (var q = 0; q < this.leafTotalPerBranch; q++) {\n",
    "                            ctx.save();\n",
    "                            if (Math.random() <= (p / this.iterations * this.leafProbaLighterMult)) {\n",
    "                                ctx.globalCompositeOperation = \"lighter\"\n",
    "                            }\n",
    "                            // Randomly pick a load leaf texture\n",
    "                            var _leafTexture = this.leafTextures[parseInt(Math.random() * this.leafTextures.length)];\n",
    "                            var u = _leafTexture.targetWidthInPixel / _leafTexture.img.naturalWidth * this.leafScale;\n",
    "\n",
    "                            ctx.scale(\n",
    "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation),\n",
    "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation)\n",
    "                            );\n",
    "                            ctx.rotate(Math.random() * Math.PI * 2);\n",
    "                            ctx.drawImage(_leafTexture.img, 0, 0);\n",
    "                            ctx.restore()\n",
    "                        }\n",
    "                    }\n",
    "                    break;\n",
    "                case \"+\":\n",
    "                    // Rotate left with variation\n",
    "                    ctx.strokeStyle = this.leftColor;\n",
    "                    ctx.rotate(this.angleMean);\n",
    "                    break;\n",
    "                case \"-\":\n",
    "                    // Rotate right with variation\n",
    "                    ctx.strokeStyle = this.rightColor;\n",
    "                    ctx.rotate(-this.angleMean);\n",
    "                    break;\n",
    "                case \"[\":\n",
    "                    ctx.save();\n",
    "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p+1));\n",
    "                    p++;\n",
    "                    w += f;\n",
    "                    ctx.lineWidth *= this.thicknessReduction;\n",
    "                    break;\n",
    "                case \"]\":\n",
    "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p-1));\n",
    "                    p--;\n",
    "                    w -= f;\n",
    "                    ctx.restore();\n",
    "                    break;\n",
    "            }\n",
    "        }\n",
    "    } catch (e) {\n",
    "        err = e\n",
    "    }\n",
    "    ctx.restore();\n",
    "    return err\n",
    "};\n",
    "\n",
    "// ----------------------------------------------------\n",
    "\n",
    "require.config({\n",
    "    paths: {\n",
    "        'dat.gui': 'https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.6/dat.gui.min'\n",
    "    }\n",
    "});\n",
    "\n",
    "(function(element) {\n",
    "    var $canvas = $('<canvas width=\"800\" height=\"500\"></canvas>');\n",
    "    var $controlContainer = $('<div style=\"position:absolute; right:0; top: 0\"></div>');\n",
    "    var $container = $('<div></div>');\n",
    "    $container\n",
    "        .append($controlContainer)\n",
    "        .append($canvas);\n",
    "    element.append($container);\n",
    "    \n",
    "    var treeGenParamValues = {\n",
    "        tp_iterations: {min: 1, max: 16, step: 1}\n",
    "    };\n",
    "    var treeParamValues = {\n",
    "        tp_angleMean: {min: 0, max: Math.PI / 4, step: Math.PI / 4 / 180},\n",
    "        tp_thickness: {min: 1, max: 150, step: .1},\n",
    "        tp_lengthReduction: {min: 0.001, max: 1},\n",
    "        tp_thicknessReduction: {min: .5, max: 1, step: .01}\n",
    "    };\n",
    "    \n",
    "    var tree = new trees2d.tree($canvas[0]);\n",
    "    var data = {\n",
    "        error: ''\n",
    "    }\n",
    "    var methods = {\n",
    "        generate: () => { data.error = tree.draw(methods.lstring) },\n",
    "        lstring: 'l[+l[+l[+l[+l[-l[-l[-l[-l]]]]][-l[-l[-l]]]][-l[+l[-l[+l[+l[-l[-l[-l[-l]]]]]]]][-l[-l[-l[-l[-l[-l]]]]]]]][-l[+l[+l[-l[-l[-l[+l[-l]]]]]][-l[+l[-l[-l[-l[-l]]]]][-l[+l[-l[-l]]][-l[-l[-l[-l[+l[+l]]]]]]]]][-l[+l[+l[-l[-l[-l[-l[-l[-l[+l[-l[-l[+l[-l[-l[-l[-l[-l[-l[-l]]]]]]]]]]]]]]]]]][-l[-l[+l[-l]][-l[-l[-l]]]]]][-l[+l[-l[-l[-l]]]][-l[-l[+l[-l[-l[-l]]]][-l[-l[+l[+l[+l]]][-l[+l][-l]]]]]]]]]][-l[+l[+l[+l[-l[-l[-l[-l[-l[-l[-l]]]]]]]][-l[+l[-l[-l]]][-l[+l][-l]]]][-l[+l[-l[-l[-l[-l[-l[-l[-l[-l[-l]]]]]]]]]][-l[+l[-l[-l[-l[-l]]]]][-l[+l][-l[+l[-l]][-l[-l]]]]]]][-l[+l[+l[+l]][-l[+l[-l[-l[-l[-l]]]]][-l[+l[-l]][-l[-l[-l[-l[-l[-l[+l]]]]]]]]]][-l[+l[-l[+l[-l[-l[-l[-l[+l][-l]]]]]][-l[+l[-l]][-l[-l[-l]]]]]][-l[+l[+l[+l[-l[-l[-l[+l]]]]][-l[-l[-l[-l]]]]][-l[+l[-l[-l[+l]]]]]][-l[+l[-l]][-l[-l[+l[-l[-l]]][-l[-l[-l[-l[-l[-l]]]]]]]]]]]]]'\n",
    "    }\n",
    "    data.error = tree.draw(methods.lstring);\n",
    "    \n",
    "    require(['dat.gui'], (dat) => {   \n",
    "        \n",
    "        var gui = new dat.GUI({autoPlace: false});\n",
    "        $controlContainer.empty()\n",
    "        $controlContainer.append(gui.domElement);\n",
    "        \n",
    "        gui.add(methods, 'lstring').onChange(() => methods.generate());\n",
    "        gui.add(methods, 'generate');\n",
    "        \n",
    "        for (var _i in treeParamValues) {\n",
    "            var p = _i.substring(3, _i.length);\n",
    "            //console.log(tree, _i, p, treeParamValues);\n",
    "            gui.add(tree, p, treeParamValues[_i].min, treeParamValues[_i].max)\n",
    "                .onChange(() => data.error = tree.draw(methods.lstring));\n",
    "        }\n",
    "        \n",
    "        gui.addColor(tree, 'leftColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
    "        gui.addColor(tree, 'rightColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
    "        \n",
    "    })\n",
    "})(element);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tree branch with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "  1\n",
      "   1\n",
      "  2\n",
      "   1\n",
      "   1\n"
     ]
    }
   ],
   "source": [
    "# Lets make a tree structure out of this\n",
    "class AccTree(Tree):\n",
    "    def __init__(self, value):\n",
    "        super(AccTree, self).__init__(value)\n",
    "    \n",
    "    def add_left(self, value):\n",
    "        if self.left:\n",
    "            self.left.value += value\n",
    "        else:\n",
    "            self.left = AccTree(value)\n",
    "        return self.left\n",
    "            \n",
    "    def add_right(self, value):\n",
    "        if self.right: \n",
    "            self.right.value += value\n",
    "        else: \n",
    "            self.right = AccTree(value)\n",
    "        return self.right\n",
    "\n",
    "\n",
    "root = AccTree(0)\n",
    "\n",
    "for dks in ['DD', 'DK', 'KK']:\n",
    "    node = root\n",
    "    for i in range(len(dks)):\n",
    "        d_or_k = dks[i]\n",
    "        if d_or_k == 'K':\n",
    "            node = node.add_left(1)\n",
    "        elif d_or_k == 'D':\n",
    "            node = node.add_right(1)\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "def print_tree(root):\n",
    "    def _print_l(node, lv): \n",
    "        print(\n",
    "            ''.join([' ']*lv*1),\n",
    "            node.value\n",
    "        )\n",
    "    root.walk(_print_l)\n",
    "    \n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l0[+l2[+l1][-l1]][-l1[-l1]]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tree_to_lstring(node):\n",
    "    if not node:\n",
    "        return\n",
    "    ret = ('l' + str(node.value))\n",
    "    if node.right:\n",
    "        ret += ('[+')\n",
    "        ret += tree_to_lstring(node.right)\n",
    "        ret += (']')\n",
    "    if node.left:\n",
    "        ret += ('[-')\n",
    "        ret += tree_to_lstring(node.left)\n",
    "        ret += (']')\n",
    "    return ret\n",
    "\n",
    "tree_to_lstring(root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l60[+l41[+l8[+l2[+l1[-l1[-l1[-l1[-l1]]]]][-l1[-l1[-l1]]]][-l6[+l1[-l1[+l1[+l1[-l1[-l1[-l1[-l1]]]]]]]][-l5[-l4[-l3[-l3[-l2[-l1]]]]]]]][-l33[+l7[+l1[-l1[-l1[-l1[+l1[-l1]]]]]][-l6[+l2[-l2[-l1[-l1[-l1]]]]][-l4[+l1[-l1[-l1]]][-l3[-l3[-l2[-l1[+l1[+l1]]]]]]]]][-l26[+l3[+l1[-l1[-l1[-l1[-l1[-l1[-l1[+l1[-l1[-l1[+l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1]]]]]]]]]]]]]]]]]][-l2[-l2[+l1[-l1]][-l1[-l1[-l1]]]]]][-l23[+l3[-l3[-l2[-l2]]]][-l20[-l13[+l3[-l2[-l1[-l1]]]][-l9[-l7[+l1[+l1[+l1]]][-l4[+l1][-l3]]]]]]]]]][-l60[+l14[+l7[+l3[-l3[-l2[-l2[-l2[-l1[-l1[-l1]]]]]]]][-l3[+l1[-l1[-l1]]][-l2[+l1][-l1]]]][-l7[+l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1]]]]]]]]]][-l6[+l1[-l1[-l1[-l1[-l1]]]]][-l4[+l1][-l3[+l1[-l1]][-l2[-l1]]]]]]][-l46[+l9[+l1[+l1]][-l8[+l1[-l1[-l1[-l1[-l1]]]]][-l6[+l1[-l1]][-l4[-l4[-l3[-l2[-l1[-l1[+l1]]]]]]]]]][-l37[+l4[-l4[+l2[-l2[-l2[-l2[-l2[+l1][-l1]]]]]][-l2[+l1[-l1]][-l1[-l1[-l1]]]]]][-l30[+l4[+l2[+l1[-l1[-l1[-l1[+l1]]]]][-l1[-l1[-l1[-l1]]]]][-l1[+l1[-l1[-l1[+l1]]]]]][-l25[+l2[-l1]][-l17[-l12[+l1[-l1[-l1]]][-l8[-l6[-l3[-l1[-l1[-l1]]]]]]]]]]]]]\n"
     ]
    }
   ],
   "source": [
    "def generate_acc_tree():\n",
    "    root = AccTree(0)\n",
    "    for k in citation_data_dict:\n",
    "        node = root\n",
    "        for _, bi in citation_data_dict[k]:\n",
    "            if bi == 0:\n",
    "                node = node.add_left(1)\n",
    "            elif bi == 1:\n",
    "                node = node.add_right(1)\n",
    "            else:\n",
    "                assert False\n",
    "    return root\n",
    "\n",
    "root = generate_acc_tree()\n",
    "root.value = max(root.left.value, root.right.value)\n",
    "print(tree_to_lstring(root))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "function LogXY(x, y) {\n",
       "  return Math.log(y) / Math.log(x);\n",
       "}\n",
       "\n",
       "// ----------------------------------------------------\n",
       "// Treer2d by Arnaud Couturier, improve by Grayson Wen\n",
       "var trees2d = {};\n",
       "trees2d.tree = function (canvas2d) {\n",
       "    this.canvas2d = canvas2d;\n",
       "    this.ctx2d = canvas2d.getContext(\"2d\");\n",
       "    this.string = \"lL[+L][-L]\";\n",
       "    this.branchTexture;\n",
       "    this.treePosY = 50;\n",
       "    this.iterations = 10;\n",
       "    this.angleMean = 0.34;\n",
       "    this.length = 29;\n",
       "    this.lengthReduction = 0.28;\n",
       "    this.thickness = 1;\n",
       "    this.thicknessReduction = 1;\n",
       "    this.thicknessRatio = 15;\n",
       "    this.rules = {\n",
       "        L: {developsInto: [\"l\", \"+lL\", \"-lL\", \"L[+LL][-LL]l\", \"[+L][-L]\", \"L[+lLL]\", \"L[-lLL]\"]},\n",
       "        l: {developsInto: [\"l\"]},\n",
       "        \"[\": {developsInto: [\"[\"]},\n",
       "        \"]\": {developsInto: [\"]\"]},\n",
       "        \"+\": {developsInto: [\"+\"]},\n",
       "        \"-\": {developsInto: [\"-\"]}\n",
       "    };\n",
       "    this.leafTextures = [];\n",
       "    this.leafScaleVariation = 0.5;\n",
       "    this.leafMinDepth = 4;\n",
       "    this.leafProba = 0.5;\n",
       "    this.leafScale = 1;\n",
       "    this.leafTotalPerBranch = 1;\n",
       "    this.leafProbaLighterMult = 0.5\n",
       "    this.shadowProba = 0;\n",
       "    this.shadowAlpha = 0.025;\n",
       "    this.shadowRadius = 0;\n",
       "    this.leftColor = '#229b71';\n",
       "    this.rightColor = '#982b6f';\n",
       "};\n",
       "trees2d.tree.prototype.addLeafTexture = function (a, b, c) {\n",
       "    this.leafTextures.push({img: a, targetWidthInPixel: b, relativeProba: c})\n",
       "};\n",
       "trees2d.tree.prototype.generateLString = function () {\n",
       "    // L-string expansion\n",
       "    var _str = this.string;\n",
       "    var lstring;\n",
       "\n",
       "    for (var m = 0; m < this.iterations; m++) {\n",
       "        lstring = \"\";\n",
       "        for (var s = 0; s < _str.length; s++) {\n",
       "            var _ch = _str[s];\n",
       "            var _developsInto = this.rules[_ch].developsInto;\n",
       "            if (_developsInto.length <= 0) {\n",
       "                continue\n",
       "            }\n",
       "            // Randomly pick one of the devInto\n",
       "            var a = _developsInto[parseInt(Math.random() * _developsInto.length)];\n",
       "\n",
       "            lstring += a\n",
       "        }\n",
       "        _str = lstring\n",
       "    }\n",
       "    return lstring\n",
       "};\n",
       "trees2d.tree.prototype.removeLeafTexture = function (a) {\n",
       "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
       "        var c = this.leafTextures[b];\n",
       "        if (c.img == a) {\n",
       "            this.leafTextures.splice(b, 1);\n",
       "            break\n",
       "        }\n",
       "    }\n",
       "};\n",
       "trees2d.tree.prototype.hasLeafTexture = function (a) {\n",
       "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
       "        var c = this.leafTextures[b];\n",
       "        if (c.img == a) {\n",
       "            return true\n",
       "        }\n",
       "    }\n",
       "    return false\n",
       "};\n",
       "trees2d.tree.prototype.totalLeafTextures = function () {\n",
       "    return this.leafTextures.length\n",
       "};\n",
       "trees2d.tree.prototype.draw = function (lstring) {\n",
       "    for (var s = 0; s < this.leafTextures.length; s++) {\n",
       "        var l = this.leafTextures[s];\n",
       "        if (!l.img.complete) {\n",
       "            return l.img.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
       "        }\n",
       "        if (l.img.naturalWidth === 0 || l.img.naturalHeight === 0) {\n",
       "            return l.img.src.substring(0, 100) + \" is not a valid image\"\n",
       "        }\n",
       "    }\n",
       "    if (this.branchTexture) {\n",
       "        if (!this.branchTexture.complete) {\n",
       "            return this.branchTexture.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
       "        }\n",
       "        if (this.branchTexture.naturalWidth === 0 || this.branchTexture.naturalHeight === 0) {\n",
       "            return this.branchTexture.src.substring(0, 100) + \" is not a valid image\"\n",
       "        }\n",
       "    }\n",
       "    var ctx = this.ctx2d;\n",
       "    var err;\n",
       "    ctx.setTransform(1, 0, 0, 1, 0, 0);\n",
       "    ctx.save();\n",
       "    try {\n",
       "\n",
       "        var w = 0.1;\n",
       "        var f = 1 / this.iterations;\n",
       "        var strokeStyle = this.branchTexture ? this.ctx2d.createPattern(this.branchTexture, \"repeat\") : 'black';\n",
       "        ctx.strokeStyle = strokeStyle;\n",
       "        ctx.lineWidth = this.thickness;\n",
       "        ctx.lineCap = \"round\";\n",
       "        var p = 1;\n",
       "        var h = 0;\n",
       "        this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p))\n",
       "        ctx.clearRect(0, 0, this.canvas2d.width, this.canvas2d.height);\n",
       "        ctx.translate(this.canvas2d.width / 2, this.canvas2d.height - this.treePosY);\n",
       "        for (var s = 0; s < lstring.length; s++) {\n",
       "            switch (lstring[s]) {\n",
       "                case \"l\":\n",
       "                    \n",
       "                    // Match the weight number afterwards\n",
       "                    var reg = lstring.slice(s+1).match(/\\d+/);\n",
       "                    if (reg) {\n",
       "                        var weight = parseInt(reg[0]);\n",
       "                        ctx.lineWidth = (weight + 1) / this.thicknessRatio + 1;\n",
       "                        s += reg[0].length;\n",
       "                    }\n",
       "                    \n",
       "                    ctx.beginPath();\n",
       "                    ctx.moveTo(0, 0);\n",
       "                    ctx.quadraticCurveTo(0, - this.length / 2, 0, -this.length);\n",
       "                    ctx.stroke();\n",
       "\n",
       "                    if (Math.random() <= this.shadowProba) {\n",
       "                        // Draw dark shadow\n",
       "                        ctx.save();\n",
       "                        ctx.globalCompositeOperation = \"source-atop\";\n",
       "                        ctx.globalAlpha = this.shadowAlpha;\n",
       "                        ctx.beginPath();\n",
       "                        ctx.arc(0, 0, this.shadowRadius, 0, Math.PI * 2, false);\n",
       "                        ctx.fill();\n",
       "                        ctx.restore()\n",
       "                    }\n",
       "                    h++;\n",
       "                    ctx.translate(0, -this.length);\n",
       "\n",
       "                    // Draw leaf here\n",
       "                    if (this.leafTextures.length > 0 && p >= this.leafMinDepth && Math.random() <= this.leafProba) {\n",
       "                        for (var q = 0; q < this.leafTotalPerBranch; q++) {\n",
       "                            ctx.save();\n",
       "                            if (Math.random() <= (p / this.iterations * this.leafProbaLighterMult)) {\n",
       "                                ctx.globalCompositeOperation = \"lighter\"\n",
       "                            }\n",
       "                            // Randomly pick a load leaf texture\n",
       "                            var _leafTexture = this.leafTextures[parseInt(Math.random() * this.leafTextures.length)];\n",
       "                            var u = _leafTexture.targetWidthInPixel / _leafTexture.img.naturalWidth * this.leafScale;\n",
       "\n",
       "                            ctx.scale(\n",
       "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation),\n",
       "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation)\n",
       "                            );\n",
       "                            ctx.rotate(Math.random() * Math.PI * 2);\n",
       "                            ctx.drawImage(_leafTexture.img, 0, 0);\n",
       "                            ctx.restore()\n",
       "                        }\n",
       "                    }\n",
       "                    break;\n",
       "                case \"+\":\n",
       "                    // Rotate left with variation\n",
       "                    ctx.strokeStyle = this.leftColor;\n",
       "                    ctx.rotate(this.angleMean);\n",
       "                    break;\n",
       "                case \"-\":\n",
       "                    // Rotate right with variation\n",
       "                    ctx.strokeStyle = this.rightColor;\n",
       "                    ctx.rotate(-this.angleMean);\n",
       "                    break;\n",
       "                case \"[\":\n",
       "                    ctx.save();\n",
       "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p+1));\n",
       "                    p++;\n",
       "                    w += f;\n",
       "                    ctx.lineWidth *= this.thicknessReduction;\n",
       "                    break;\n",
       "                case \"]\":\n",
       "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p-1));\n",
       "                    p--;\n",
       "                    w -= f;\n",
       "                    ctx.restore();\n",
       "                    break;\n",
       "            }\n",
       "        }\n",
       "    } catch (e) {\n",
       "        err = e\n",
       "    }\n",
       "    ctx.restore();\n",
       "    return err\n",
       "};\n",
       "\n",
       "// ----------------------------------------------------\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        'dat.gui': 'https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.6/dat.gui.min'\n",
       "    }\n",
       "});\n",
       "\n",
       "(function(element) {\n",
       "    var $canvas = $('<canvas width=\"800\" height=\"500\"></canvas>');\n",
       "    var $controlContainer = $('<div style=\"position:absolute; right:0; top: 0\"></div>');\n",
       "    var $container = $('<div></div>');\n",
       "    $container\n",
       "        .append($controlContainer)\n",
       "        .append($canvas);\n",
       "    element.append($container);\n",
       "    \n",
       "    var treeGenParamValues = {\n",
       "        tp_iterations: {min: 1, max: 16, step: 1}\n",
       "    };\n",
       "    var treeParamValues = {\n",
       "        tp_angleMean: {min: 0, max: Math.PI / 4, step: Math.PI / 4 / 180},\n",
       "        tp_lengthReduction: {min: 0.001, max: 1},\n",
       "        tp_thicknessRatio: {min: 1, max: 100, step: 1}\n",
       "    };\n",
       "    \n",
       "    var tree = new trees2d.tree($canvas[0]);\n",
       "    var data = {\n",
       "        error: ''\n",
       "    }\n",
       "    var methods = {\n",
       "        generate: () => { data.error = tree.draw(methods.lstring) },\n",
       "        lstring: 'l60[+l41[+l8[+l2[+l1[-l1[-l1[-l1[-l1]]]]][-l1[-l1[-l1]]]][-l6[+l1[-l1[+l1[+l1[-l1[-l1[-l1[-l1]]]]]]]][-l5[-l4[-l3[-l3[-l2[-l1]]]]]]]][-l33[+l7[+l1[-l1[-l1[-l1[+l1[-l1]]]]]][-l6[+l2[-l2[-l1[-l1[-l1]]]]][-l4[+l1[-l1[-l1]]][-l3[-l3[-l2[-l1[+l1[+l1]]]]]]]]][-l26[+l3[+l1[-l1[-l1[-l1[-l1[-l1[-l1[+l1[-l1[-l1[+l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1]]]]]]]]]]]]]]]]]][-l2[-l2[+l1[-l1]][-l1[-l1[-l1]]]]]][-l23[+l3[-l3[-l2[-l2]]]][-l20[-l13[+l3[-l2[-l1[-l1]]]][-l9[-l7[+l1[+l1[+l1]]][-l4[+l1][-l3]]]]]]]]]][-l60[+l14[+l7[+l3[-l3[-l2[-l2[-l2[-l1[-l1[-l1]]]]]]]][-l3[+l1[-l1[-l1]]][-l2[+l1][-l1]]]][-l7[+l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1]]]]]]]]]][-l6[+l1[-l1[-l1[-l1[-l1]]]]][-l4[+l1][-l3[+l1[-l1]][-l2[-l1]]]]]]][-l46[+l9[+l1[+l1]][-l8[+l1[-l1[-l1[-l1[-l1]]]]][-l6[+l1[-l1]][-l4[-l4[-l3[-l2[-l1[-l1[+l1]]]]]]]]]][-l37[+l4[-l4[+l2[-l2[-l2[-l2[-l2[+l1][-l1]]]]]][-l2[+l1[-l1]][-l1[-l1[-l1]]]]]][-l30[+l4[+l2[+l1[-l1[-l1[-l1[+l1]]]]][-l1[-l1[-l1[-l1]]]]][-l1[+l1[-l1[-l1[+l1]]]]]][-l25[+l2[-l1]][-l17[-l12[+l1[-l1[-l1]]][-l8[-l6[-l3[-l1[-l1[-l1]]]]]]]]]]]]]'\n",
       "    }\n",
       "    data.error = tree.draw(methods.lstring);\n",
       "    \n",
       "    require(['dat.gui'], (dat) => {   \n",
       "        \n",
       "        var gui = new dat.GUI({autoPlace: false});\n",
       "        $controlContainer.empty()\n",
       "        $controlContainer.append(gui.domElement);\n",
       "        \n",
       "        gui.add(methods, 'lstring').onChange(() => methods.generate());\n",
       "        gui.add(methods, 'generate');\n",
       "        \n",
       "        for (var _i in treeParamValues) {\n",
       "            var p = _i.substring(3, _i.length);\n",
       "            //console.log(tree, _i, p, treeParamValues);\n",
       "            gui.add(tree, p, treeParamValues[_i].min, treeParamValues[_i].max)\n",
       "                .onChange(() => data.error = tree.draw(methods.lstring));\n",
       "        }\n",
       "        \n",
       "        gui.addColor(tree, 'leftColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
       "        gui.addColor(tree, 'rightColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
       "        \n",
       "    })\n",
       "})(element);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "function LogXY(x, y) {\n",
    "  return Math.log(y) / Math.log(x);\n",
    "}\n",
    "\n",
    "// ----------------------------------------------------\n",
    "// Treer2d by Arnaud Couturier, improve by Grayson Wen\n",
    "var trees2d = {};\n",
    "trees2d.tree = function (canvas2d) {\n",
    "    this.canvas2d = canvas2d;\n",
    "    this.ctx2d = canvas2d.getContext(\"2d\");\n",
    "    this.string = \"lL[+L][-L]\";\n",
    "    this.branchTexture;\n",
    "    this.treePosY = 50;\n",
    "    this.iterations = 10;\n",
    "    this.angleMean = 0.34;\n",
    "    this.length = 29;\n",
    "    this.lengthReduction = 0.28;\n",
    "    this.thickness = 1;\n",
    "    this.thicknessReduction = 1;\n",
    "    this.thicknessRatio = 15;\n",
    "    this.rules = {\n",
    "        L: {developsInto: [\"l\", \"+lL\", \"-lL\", \"L[+LL][-LL]l\", \"[+L][-L]\", \"L[+lLL]\", \"L[-lLL]\"]},\n",
    "        l: {developsInto: [\"l\"]},\n",
    "        \"[\": {developsInto: [\"[\"]},\n",
    "        \"]\": {developsInto: [\"]\"]},\n",
    "        \"+\": {developsInto: [\"+\"]},\n",
    "        \"-\": {developsInto: [\"-\"]}\n",
    "    };\n",
    "    this.leafTextures = [];\n",
    "    this.leafScaleVariation = 0.5;\n",
    "    this.leafMinDepth = 4;\n",
    "    this.leafProba = 0.5;\n",
    "    this.leafScale = 1;\n",
    "    this.leafTotalPerBranch = 1;\n",
    "    this.leafProbaLighterMult = 0.5\n",
    "    this.shadowProba = 0;\n",
    "    this.shadowAlpha = 0.025;\n",
    "    this.shadowRadius = 0;\n",
    "    this.leftColor = '#229b71';\n",
    "    this.rightColor = '#982b6f';\n",
    "};\n",
    "trees2d.tree.prototype.addLeafTexture = function (a, b, c) {\n",
    "    this.leafTextures.push({img: a, targetWidthInPixel: b, relativeProba: c})\n",
    "};\n",
    "trees2d.tree.prototype.generateLString = function () {\n",
    "    // L-string expansion\n",
    "    var _str = this.string;\n",
    "    var lstring;\n",
    "\n",
    "    for (var m = 0; m < this.iterations; m++) {\n",
    "        lstring = \"\";\n",
    "        for (var s = 0; s < _str.length; s++) {\n",
    "            var _ch = _str[s];\n",
    "            var _developsInto = this.rules[_ch].developsInto;\n",
    "            if (_developsInto.length <= 0) {\n",
    "                continue\n",
    "            }\n",
    "            // Randomly pick one of the devInto\n",
    "            var a = _developsInto[parseInt(Math.random() * _developsInto.length)];\n",
    "\n",
    "            lstring += a\n",
    "        }\n",
    "        _str = lstring\n",
    "    }\n",
    "    return lstring\n",
    "};\n",
    "trees2d.tree.prototype.removeLeafTexture = function (a) {\n",
    "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
    "        var c = this.leafTextures[b];\n",
    "        if (c.img == a) {\n",
    "            this.leafTextures.splice(b, 1);\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "};\n",
    "trees2d.tree.prototype.hasLeafTexture = function (a) {\n",
    "    for (var b = 0; b < this.leafTextures.length; b++) {\n",
    "        var c = this.leafTextures[b];\n",
    "        if (c.img == a) {\n",
    "            return true\n",
    "        }\n",
    "    }\n",
    "    return false\n",
    "};\n",
    "trees2d.tree.prototype.totalLeafTextures = function () {\n",
    "    return this.leafTextures.length\n",
    "};\n",
    "trees2d.tree.prototype.draw = function (lstring) {\n",
    "    for (var s = 0; s < this.leafTextures.length; s++) {\n",
    "        var l = this.leafTextures[s];\n",
    "        if (!l.img.complete) {\n",
    "            return l.img.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
    "        }\n",
    "        if (l.img.naturalWidth === 0 || l.img.naturalHeight === 0) {\n",
    "            return l.img.src.substring(0, 100) + \" is not a valid image\"\n",
    "        }\n",
    "    }\n",
    "    if (this.branchTexture) {\n",
    "        if (!this.branchTexture.complete) {\n",
    "            return this.branchTexture.src.substring(0, 100) + \" is not completely loaded, wait until all textures are loaded\"\n",
    "        }\n",
    "        if (this.branchTexture.naturalWidth === 0 || this.branchTexture.naturalHeight === 0) {\n",
    "            return this.branchTexture.src.substring(0, 100) + \" is not a valid image\"\n",
    "        }\n",
    "    }\n",
    "    var ctx = this.ctx2d;\n",
    "    var err;\n",
    "    ctx.setTransform(1, 0, 0, 1, 0, 0);\n",
    "    ctx.save();\n",
    "    try {\n",
    "\n",
    "        var w = 0.1;\n",
    "        var f = 1 / this.iterations;\n",
    "        var strokeStyle = this.branchTexture ? this.ctx2d.createPattern(this.branchTexture, \"repeat\") : 'black';\n",
    "        ctx.strokeStyle = strokeStyle;\n",
    "        ctx.lineWidth = this.thickness;\n",
    "        ctx.lineCap = \"round\";\n",
    "        var p = 1;\n",
    "        var h = 0;\n",
    "        this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p))\n",
    "        ctx.clearRect(0, 0, this.canvas2d.width, this.canvas2d.height);\n",
    "        ctx.translate(this.canvas2d.width / 2, this.canvas2d.height - this.treePosY);\n",
    "        for (var s = 0; s < lstring.length; s++) {\n",
    "            switch (lstring[s]) {\n",
    "                case \"l\":\n",
    "                    \n",
    "                    // Match the weight number afterwards\n",
    "                    var reg = lstring.slice(s+1).match(/\\d+/);\n",
    "                    if (reg) {\n",
    "                        var weight = parseInt(reg[0]);\n",
    "                        ctx.lineWidth = (weight + 1) / this.thicknessRatio + 1;\n",
    "                        s += reg[0].length;\n",
    "                    }\n",
    "                    \n",
    "                    ctx.beginPath();\n",
    "                    ctx.moveTo(0, 0);\n",
    "                    ctx.quadraticCurveTo(0, - this.length / 2, 0, -this.length);\n",
    "                    ctx.stroke();\n",
    "\n",
    "                    if (Math.random() <= this.shadowProba) {\n",
    "                        // Draw dark shadow\n",
    "                        ctx.save();\n",
    "                        ctx.globalCompositeOperation = \"source-atop\";\n",
    "                        ctx.globalAlpha = this.shadowAlpha;\n",
    "                        ctx.beginPath();\n",
    "                        ctx.arc(0, 0, this.shadowRadius, 0, Math.PI * 2, false);\n",
    "                        ctx.fill();\n",
    "                        ctx.restore()\n",
    "                    }\n",
    "                    h++;\n",
    "                    ctx.translate(0, -this.length);\n",
    "\n",
    "                    // Draw leaf here\n",
    "                    if (this.leafTextures.length > 0 && p >= this.leafMinDepth && Math.random() <= this.leafProba) {\n",
    "                        for (var q = 0; q < this.leafTotalPerBranch; q++) {\n",
    "                            ctx.save();\n",
    "                            if (Math.random() <= (p / this.iterations * this.leafProbaLighterMult)) {\n",
    "                                ctx.globalCompositeOperation = \"lighter\"\n",
    "                            }\n",
    "                            // Randomly pick a load leaf texture\n",
    "                            var _leafTexture = this.leafTextures[parseInt(Math.random() * this.leafTextures.length)];\n",
    "                            var u = _leafTexture.targetWidthInPixel / _leafTexture.img.naturalWidth * this.leafScale;\n",
    "\n",
    "                            ctx.scale(\n",
    "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation),\n",
    "                                u * (1 + (Math.random() * 2 - 1) * this.leafScaleVariation)\n",
    "                            );\n",
    "                            ctx.rotate(Math.random() * Math.PI * 2);\n",
    "                            ctx.drawImage(_leafTexture.img, 0, 0);\n",
    "                            ctx.restore()\n",
    "                        }\n",
    "                    }\n",
    "                    break;\n",
    "                case \"+\":\n",
    "                    // Rotate left with variation\n",
    "                    ctx.strokeStyle = this.leftColor;\n",
    "                    ctx.rotate(this.angleMean);\n",
    "                    break;\n",
    "                case \"-\":\n",
    "                    // Rotate right with variation\n",
    "                    ctx.strokeStyle = this.rightColor;\n",
    "                    ctx.rotate(-this.angleMean);\n",
    "                    break;\n",
    "                case \"[\":\n",
    "                    ctx.save();\n",
    "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p+1));\n",
    "                    p++;\n",
    "                    w += f;\n",
    "                    ctx.lineWidth *= this.thicknessReduction;\n",
    "                    break;\n",
    "                case \"]\":\n",
    "                    this.length = 65 * Math.pow(this.lengthReduction, 0.1*(p-1));\n",
    "                    p--;\n",
    "                    w -= f;\n",
    "                    ctx.restore();\n",
    "                    break;\n",
    "            }\n",
    "        }\n",
    "    } catch (e) {\n",
    "        err = e\n",
    "    }\n",
    "    ctx.restore();\n",
    "    return err\n",
    "};\n",
    "\n",
    "// ----------------------------------------------------\n",
    "\n",
    "require.config({\n",
    "    paths: {\n",
    "        'dat.gui': 'https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.6/dat.gui.min'\n",
    "    }\n",
    "});\n",
    "\n",
    "(function(element) {\n",
    "    var $canvas = $('<canvas width=\"800\" height=\"500\"></canvas>');\n",
    "    var $controlContainer = $('<div style=\"position:absolute; right:0; top: 0\"></div>');\n",
    "    var $container = $('<div></div>');\n",
    "    $container\n",
    "        .append($controlContainer)\n",
    "        .append($canvas);\n",
    "    element.append($container);\n",
    "    \n",
    "    var treeGenParamValues = {\n",
    "        tp_iterations: {min: 1, max: 16, step: 1}\n",
    "    };\n",
    "    var treeParamValues = {\n",
    "        tp_angleMean: {min: 0, max: Math.PI / 4, step: Math.PI / 4 / 180},\n",
    "        tp_lengthReduction: {min: 0.001, max: 1},\n",
    "        tp_thicknessRatio: {min: 1, max: 100, step: 1}\n",
    "    };\n",
    "    \n",
    "    var tree = new trees2d.tree($canvas[0]);\n",
    "    var data = {\n",
    "        error: ''\n",
    "    }\n",
    "    var methods = {\n",
    "        generate: () => { data.error = tree.draw(methods.lstring) },\n",
    "        lstring: 'l60[+l41[+l8[+l2[+l1[-l1[-l1[-l1[-l1]]]]][-l1[-l1[-l1]]]][-l6[+l1[-l1[+l1[+l1[-l1[-l1[-l1[-l1]]]]]]]][-l5[-l4[-l3[-l3[-l2[-l1]]]]]]]][-l33[+l7[+l1[-l1[-l1[-l1[+l1[-l1]]]]]][-l6[+l2[-l2[-l1[-l1[-l1]]]]][-l4[+l1[-l1[-l1]]][-l3[-l3[-l2[-l1[+l1[+l1]]]]]]]]][-l26[+l3[+l1[-l1[-l1[-l1[-l1[-l1[-l1[+l1[-l1[-l1[+l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1]]]]]]]]]]]]]]]]]][-l2[-l2[+l1[-l1]][-l1[-l1[-l1]]]]]][-l23[+l3[-l3[-l2[-l2]]]][-l20[-l13[+l3[-l2[-l1[-l1]]]][-l9[-l7[+l1[+l1[+l1]]][-l4[+l1][-l3]]]]]]]]]][-l60[+l14[+l7[+l3[-l3[-l2[-l2[-l2[-l1[-l1[-l1]]]]]]]][-l3[+l1[-l1[-l1]]][-l2[+l1][-l1]]]][-l7[+l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1[-l1]]]]]]]]]][-l6[+l1[-l1[-l1[-l1[-l1]]]]][-l4[+l1][-l3[+l1[-l1]][-l2[-l1]]]]]]][-l46[+l9[+l1[+l1]][-l8[+l1[-l1[-l1[-l1[-l1]]]]][-l6[+l1[-l1]][-l4[-l4[-l3[-l2[-l1[-l1[+l1]]]]]]]]]][-l37[+l4[-l4[+l2[-l2[-l2[-l2[-l2[+l1][-l1]]]]]][-l2[+l1[-l1]][-l1[-l1[-l1]]]]]][-l30[+l4[+l2[+l1[-l1[-l1[-l1[+l1]]]]][-l1[-l1[-l1[-l1]]]]][-l1[+l1[-l1[-l1[+l1]]]]]][-l25[+l2[-l1]][-l17[-l12[+l1[-l1[-l1]]][-l8[-l6[-l3[-l1[-l1[-l1]]]]]]]]]]]]]'\n",
    "    }\n",
    "    data.error = tree.draw(methods.lstring);\n",
    "    \n",
    "    require(['dat.gui'], (dat) => {   \n",
    "        \n",
    "        var gui = new dat.GUI({autoPlace: false});\n",
    "        $controlContainer.empty()\n",
    "        $controlContainer.append(gui.domElement);\n",
    "        \n",
    "        gui.add(methods, 'lstring').onChange(() => methods.generate());\n",
    "        gui.add(methods, 'generate');\n",
    "        \n",
    "        for (var _i in treeParamValues) {\n",
    "            var p = _i.substring(3, _i.length);\n",
    "            //console.log(tree, _i, p, treeParamValues);\n",
    "            gui.add(tree, p, treeParamValues[_i].min, treeParamValues[_i].max)\n",
    "                .onChange(() => data.error = tree.draw(methods.lstring));\n",
    "        }\n",
    "        \n",
    "        gui.addColor(tree, 'leftColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
    "        gui.addColor(tree, 'rightColor').onChange(() => data.error = tree.draw(methods.lstring));\n",
    "        \n",
    "    })\n",
    "})(element);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
